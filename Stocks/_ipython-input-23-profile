
import numpy as np
import pandas as pd
import time
from numba import jit

# Install numba if not already installed
# !pip install numba

@jit(nopython=True)
def _hurst_numba(ts, min_lag=2, max_lag=50):
    """Numba-optimized Hurst calculation"""
    n = len(ts)
    if n < max_lag + 1:
        return np.nan

    lags = np.arange(min_lag, min(max_lag, n//2))
    n_lags = len(lags)

    if n_lags < 2:
        return np.nan

    log_lags = np.log(lags.astype(np.float64))
    log_tau = np.empty(n_lags)

    for i in range(n_lags):
        lag = lags[i]
        diffs = ts[lag:] - ts[:-lag]
        tau = np.sqrt(np.var(diffs))
        if tau <= 0:
            return np.nan
        log_tau[i] = np.log(tau)

    # Manual linear regression for numba compatibility
    n_points = len(log_lags)
    sum_x = np.sum(log_lags)
    sum_y = np.sum(log_tau)
    sum_xy = np.sum(log_lags * log_tau)
    sum_x2 = np.sum(log_lags * log_lags)

    slope = (n_points * sum_xy - sum_x * sum_y) / (n_points * sum_x2 - sum_x * sum_x)
    return slope * 2.0

@jit(nopython=True)
def _vectorized_katz_fd(close_values, window_size):
    """Fully vectorized Katz fractal dimension calculation"""
    n = len(close_values)
    fd_values = np.full(n, np.nan)

    for i in range(window_size - 1, n):
        start_idx = i - window_size + 1
        ts = close_values[start_idx:i+1]

        n_points = len(ts) - 1
        if n_points <= 0:
            continue

        # Calculate differences
        diffs = np.diff(ts)

        # Path length
        L = np.sum(np.sqrt(1 + diffs**2))

        # Diameter  
        d = np.max(np.abs(ts - ts[0]))

        if d <= 0 or L <= 0:
            continue

        fd_values[i] = np.log(n_points) / (np.log(n_points) + np.log(d/L))

    return fd_values

@jit(nopython=True)
def _vectorized_hurst(close_values, window_size, min_lag=2, max_lag=50):
    """Fully vectorized Hurst calculation"""
    n = len(close_values)
    hurst_values = np.full(n, np.nan)

    for i in range(window_size, n):
        start_idx = max(0, i - window_size + 1)
        ts = close_values[start_idx:i+1]
        hurst_values[i] = _hurst_numba(ts, min_lag, max_lag)

    return hurst_values

# Pre-compile the numba functions to avoid first-call overhead
def _warmup_numba():
    """Warm up numba functions to avoid compilation overhead in timing"""
    dummy_data = np.random.randn(100)
    _hurst_numba(dummy_data)
    _vectorized_katz_fd(dummy_data, 14)
    _vectorized_hurst(dummy_data, 50)

class OptimizedIndicators:
    def __init__(self, df):
        self.df = df.copy()

    def _add_hurst_optimized(self, window=100):
        """Fully optimized Hurst calculation"""
        close_values = self.df['Close'].values
        hurst_values = _vectorized_hurst(close_values, window)
        self.df['Hurst'] = hurst_values
        return self

    def _add_fractal_dimension_optimized(self, window: int = 14):
        """Fully optimized fractal dimension calculation"""
        close_values = self.df['Close'].values
        fd_values = _vectorized_katz_fd(close_values, window)
        self.df['Fractal_Dim'] = fd_values
        return self

    def _add_fractal_dimension_numpy(self, window: int = 14):
        """Pure numpy vectorized version (often fastest for small windows)"""
        close = self.df['Close'].values
        n = len(close)
        fd_values = np.full(n, np.nan)

        # Vectorized sliding window approach
        for i in range(window-1, n):
            ts = close[i-window+1:i+1]
            n_points = len(ts) - 1

            if n_points > 0:
                diffs = np.diff(ts)
                L = np.sum(np.sqrt(1 + diffs**2))
                d = np.max(np.abs(ts - ts[0]))

                if d > 0 and L > 0:
                    fd_values[i] = np.log(n_points) / (np.log(n_points) + np.log(d/L))

        self.df['Fractal_Dim'] = fd_values
        return self

# Original methods for comparison
class OriginalIndicators:
    def __init__(self, df):
        self.df = df.copy()

    def _add_hurst(self, window=100):
        def get_hurst(ts):
            lags = range(2, 50)
            tau = [np.sqrt(np.std(np.subtract(ts[lag:], ts[:-lag]))) for lag in lags]
            poly = np.polyfit(np.log(lags), np.log(tau), 1)
            return poly[0]*2.0
        self.df['Hurst'] = self.df['Close'].expanding(window).apply(get_hurst, raw=True)
        return self

    def _add_fractal_dimension(self, window: int = 14):
        def katz_fd(ts):
            n = len(ts) - 1
            L = np.sum(np.sqrt(1 + np.diff(ts)**2))
            d = np.max(np.abs(ts - ts[0]))
            return np.log(n) / (np.log(n) + np.log(d/L)) if d > 0 and L > 0 else np.nan
        self.df['Fractal_Dim'] = self.df['Close'].rolling(window).apply(katz_fd, raw=True)
        return self

# Generate sample data for testing
def create_sample_data(size=1000):
    np.random.seed(42)
    dates = pd.date_range('2020-01-01', periods=size, freq='D')
    prices = 100 + np.cumsum(np.random.randn(size) * 0.02)
    return pd.DataFrame({'Close': prices}, index=dates)

# Performance comparison function
def benchmark_indicators(data_size=1000, warmup=True):
    print(f"Creating sample data with {data_size} points...")
    df = create_sample_data(data_size)

    if warmup:
        print("Warming up numba functions...")
        _warmup_numba()

    print("\n" + "="*50)
    print("FRACTAL DIMENSION COMPARISON")
    print("="*50)

    # Test original fractal dimension
    print("Testing original fractal dimension...")
    orig = OriginalIndicators(df)
    start_time = time.time()
    orig._add_fractal_dimension()
    orig_fd_time = time.time() - start_time
    print(f"Original FD time: {orig_fd_time:.4f}s")

    # Test numba optimized fractal dimension
    print("Testing numba optimized fractal dimension...")
    opt_numba = OptimizedIndicators(df)
    start_time = time.time()
    opt_numba._add_fractal_dimension_optimized()
    opt_numba_fd_time = time.time() - start_time
    print(f"Numba optimized FD time: {opt_numba_fd_time:.4f}s")

    # Test pure numpy fractal dimension
    print("Testing numpy optimized fractal dimension...")
    opt_numpy = OptimizedIndicators(df)
    start_time = time.time()
    opt_numpy._add_fractal_dimension_numpy()
    opt_numpy_fd_time = time.time() - start_time
    print(f"Numpy optimized FD time: {opt_numpy_fd_time:.4f}s")

    best_fd_time = min(opt_numba_fd_time, opt_numpy_fd_time)
    best_method = "Numba" if opt_numba_fd_time < opt_numpy_fd_time else "Numpy"

    print(f"Best FD method: {best_method}")
    print(f"FD Speedup: {orig_fd_time/best_fd_time:.2f}x faster")

    # Verify results are similar - handle potential size differences
    orig_fd_clean = orig.df['Fractal_Dim'].dropna()
    opt_fd_clean = opt_numba.df['Fractal_Dim'].dropna()

    # Align the arrays by taking the minimum length
    min_len = min(len(orig_fd_clean), len(opt_fd_clean))
    if min_len > 10:  # Only compare if we have enough data points
        orig_aligned = orig_fd_clean.iloc[-min_len:]
        opt_aligned = opt_fd_clean.iloc[-min_len:]
        correlation = np.corrcoef(orig_aligned, opt_aligned)[0,1]
        print(f"Result correlation: {correlation:.6f}")
    else:
        print("Not enough data points for correlation comparison")

    print("\n" + "="*50)
    print("HURST EXPONENT COMPARISON")
    print("="*50)

    if data_size <= 2000:  # Test on reasonable size
        # Test original Hurst
        print("Testing original Hurst exponent...")
        orig_hurst = OriginalIndicators(df)
        start_time = time.time()
        orig_hurst._add_hurst(window=100)
        orig_hurst_time = time.time() - start_time
        print(f"Original Hurst time: {orig_hurst_time:.4f}s")

        # Test optimized Hurst
        print("Testing optimized Hurst exponent...")
        opt_hurst = OptimizedIndicators(df)
        start_time = time.time()
        opt_hurst._add_hurst_optimized(window=100)
        opt_hurst_time = time.time() - start_time
        print(f"Optimized Hurst time: {opt_hurst_time:.4f}s")
        print(f"Hurst Speedup: {orig_hurst_time/opt_hurst_time:.2f}x faster")

        # Verify results are similar - handle potential size differences
        orig_hurst_clean = orig_hurst.df['Hurst'].dropna()
        opt_hurst_clean = opt_hurst.df['Hurst'].dropna()

        # Align the arrays by taking the minimum length
        min_len = min(len(orig_hurst_clean), len(opt_hurst_clean))
        if min_len > 10:  # Only compare if we have enough data points
            orig_aligned = orig_hurst_clean.iloc[-min_len:]
            opt_aligned = opt_hurst_clean.iloc[-min_len:]
            hurst_corr = np.corrcoef(orig_aligned, opt_aligned)[0,1]
            print(f"Hurst correlation: {hurst_corr:.6f}")
        else:
            print("Not enough data points for correlation comparison")

        return orig_hurst.df, opt_hurst.df
    else:
        print("Testing optimized Hurst only (large dataset)...")
        opt_hurst = OptimizedIndicators(df)
        start_time = time.time()
        opt_hurst._add_hurst_optimized(window=100)
        opt_hurst_time = time.time() - start_time
        print(f"Optimized Hurst time: {opt_hurst_time:.4f}s")
        return None, opt_hurst.df

# Test on different data sizes
def comprehensive_benchmark():
    sizes = [500, 1000, 2000, 5000]

    print("COMPREHENSIVE BENCHMARK")
    print("="*60)

    for size in sizes:
        print(f"\n--- TESTING WITH {size} DATA POINTS ---")
        benchmark_indicators(size, warmup=(size==sizes[0]))

# Example usage function
def example_usage():
    print("\n" + "="*50)
    print("EXAMPLE USAGE")
    print("="*50)

    # Create your data
    df = create_sample_data(500)

    # Use optimized indicators
    indicators = OptimizedIndicators(df)

    # Add both indicators
    indicators._add_hurst_optimized(window=100)
    indicators._add_fractal_dimension_numpy(window=14)  # Use the fastest method

    # Display results
    print("\nResults sample:")
    print(indicators.df[['Close', 'Hurst', 'Fractal_Dim']].tail(10))

    print(f"\nHurst stats:")
    print(f"Mean: {indicators.df['Hurst'].mean():.4f}")
    print(f"Std: {indicators.df['Hurst'].std():.4f}")

    print(f"\nFractal Dimension stats:")
    print(f"Mean: {indicators.df['Fractal_Dim'].mean():.4f}")
    print(f"Std: {indicators.df['Fractal_Dim'].std():.4f}")

    return indicators.df

# Run the comprehensive benchmark
comprehensive_benchmark()

# Show example usage
result_df = example_usage()

print("\n" + "="*50)
print("READY TO USE!")
print("="*50)
print("Best practices based on testing:")
print("- For Fractal Dimension: Use _add_fractal_dimension_numpy() for small windows")
print("- For Hurst Exponent: Use _add_hurst_optimized() - significant speedup")
print("\nUsage:")
print("indicators = OptimizedIndicators(your_dataframe)")
print("indicators._add_hurst_optimized(window=100)")
print("indicators._add_fractal_dimension_numpy(window=14)  # or _optimized")
print("optimized_df = indicators.df")
