# run.py
import initialize
import argparse
import re
import time
import torch
import torch.nn.functional as F
import numpy as np
import pandas as pd
# Import project modules
import config
import data_handler
import audit_logger
from feature_engineering import FeatureCalculator
import models

def validate_ticker(ticker):
    """Sanitize and validate a stock ticker format."""
    ticker = ticker.strip().upper()
    if re.match(r'^[A-Z]{1,5}(\.[A-Z]{1,2})?$', ticker):
        return ticker
    raise argparse.ArgumentTypeError(f"'{ticker}' is not a valid ticker format.")

def run_single_ticker_analysis(ticker: str):
    """Orchestrates the analysis for a single ticker and logs the result."""
    import predictors # lazy import to suppress tensorflow warnings
    print(f"\n{'='*20} Processing Ticker: {ticker} {'='*20}")
    
    # 1. Load Data
    df_raw = data_handler.get_stock_data(
        ticker=ticker,
        start_date=config.START_DATE,
        end_date=config.END_DATE
    )
    if df_raw is None or len(df_raw) < 252:
        print(f"Insufficient data for {ticker}. Skipping.")
        return
    
    # Explicitly create a deep, writable copy of the dataframe.
    df_writable = df_raw.copy()

    # 2. Feature Engineering
    # Pass the writable copy to the calculator
    feature_calculator = FeatureCalculator(df_writable)
    df_features = feature_calculator.add_all_features()

    df_features.replace([np.inf, -np.inf], np.nan, inplace=True)
    initial_rows = len(df_features)
    df_features.dropna(inplace=True)
    print(f"Data cleaned: Removed {initial_rows - len(df_features)} rows with NaN/inf values.")
    
    # 3. Define Target and Split Data
    df_features['UpNext'] = (df_features['Close'].shift(-1) > df_features['Close']).astype(int)
    df_features.dropna(inplace=True)

    train_size = int(len(df_features) * config.TRAIN_SPLIT_RATIO)
    train_df = df_features.iloc[:train_size]
    test_df = df_features.iloc[train_size:]

    feature_cols = [col for col in config.FEATURE_COLS if col != 'mc_trend_strength']
    X_train, y_train = train_df[feature_cols], train_df['UpNext']
    X_test, y_test = test_df[feature_cols], test_df['UpNext']
    
#########################
    # 4. Train Models & Predict (simplified for brevity)
    xgb_model = models.train_xgboost(X_train, y_train)
    xgb_proba = xgb_model.predict_proba(X_test)[:, 1]

    # Transformer
    X_seq, y_seq = models.prepare_sequences(
        df_features[config.FEATURE_COLS].values, 
        df_features['UpNext'].values, 
        config.TRANSFORMER_SEQUENCE_WINDOW
    )
    # Align sequence data with train/test split
    seq_train_size = len(train_df) - config.TRANSFORMER_SEQUENCE_WINDOW
    X_seq_train, y_seq_train = X_seq[:seq_train_size], y_seq[:seq_train_size]
    X_seq_test, y_seq_test = X_seq[seq_train_size:], y_seq[seq_train_size:]
    
    transformer_model, device = models.train_transformer(X_seq_train, y_seq_train) # Capture device
        
    # 5a. Evaluate and Ensemble
    transformer_model.eval()
    with torch.no_grad():
        # Move test data to the same device for inference
        test_tensor = torch.tensor(X_seq_test).to(device)
        test_outputs = transformer_model(test_tensor)
        # Move probabilities back to CPU for numpy conversion
        trans_proba = F.softmax(test_outputs, dim=1).cpu().numpy()[:, 1]

    min_len = min(len(xgb_proba), len(trans_proba))
    ensemble_proba = (xgb_proba[:min_len] + trans_proba[:min_len]) / 2

    # 5b. Generate Final Monte-Carlo Signal
    print("Generating final Monte-Carlo signal...")
    mc_final_signal = {}
    try:
        # Use the full featured dataframe for the final signal generation
        mc_filter = predictors.MonteCarloTrendFilter(**config.MC_FILTER_PARAMS)
        mc_filter.fit(df_features) # Fit on all available data
        mc_final_signal = mc_filter.predict(horizon=21) # Predict for the next month
        print(f"MC Signal: Trend Strength={mc_final_signal.get('trend_strength', 0):.3f}")
    except Exception as e:
        print(f"Could not generate Monte-Carlo signal: {e}")


    # 6. Generate Metrics & Importance
    print("Calculating SHAP values...")
    shap_values = models.get_shap_importance(xgb_model, X_test)

    kelly_fraction = 2 * xgb_proba[-1] - 1 if len(xgb_proba) > 0 else 0.0
    accuracy = (ensemble_proba > 0.5).astype(int) == y_test[:len(ensemble_proba)].values
    accuracy = accuracy.mean()
    
    metrics = {"accuracy": accuracy,
               "kelly_fraction": kelly_fraction,
               "mc_trend_strength": mc_final_signal.get('trend_strength'),
               "mc_up_prob": mc_final_signal.get('up_prob'),
               "mc_down_prob": mc_final_signal.get('down_prob'),
               "mc_ci": mc_final_signal.get('ci', [None, None]),
               "mc_simulated_slopes": mc_final_signal.get('simulated_slopes', [])         
    }

#########################
    
    # 7. Log Results for Auditing
    audit_logger.log_analysis_result(
        ticker=ticker,
        model_name="XGBoost_Ensemble_v1",
        run_config=config.XGB_PARAMS,
        predictions={"probabilities": xgb_proba.tolist()}, # Log the raw probabilities
        metrics=metrics,
        shap_importance={
            'features': X_test.columns.tolist(),
            'values': shap_values.tolist()
        }
    )
    print(f"Analysis for {ticker} complete. Results logged.")

def main():
    parser = argparse.ArgumentParser(description="Run the stock analysis pipeline and log results.")
    parser.add_argument(
        'tickers',
        metavar='TICKER',
        type=validate_ticker,
        nargs='+',
        help='A space-separated list of stock tickers to analyze.'
    )
    args = parser.parse_args()
    
    audit_logger.setup_audit_db()

    for ticker in args.tickers:
        try:
            run_single_ticker_analysis(ticker)
            time.sleep(1) 
        except Exception as e:
            print(f"!!! FAILED to process {ticker}: {e}")
            continue

if __name__ == '__main__':
    main()