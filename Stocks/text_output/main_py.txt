from pathlib import Path
from datetime import datetime, timedelta
import pandas as pd
import numpy as np
import yfinance as yf
from xgboost import XGBClassifier
from scipy.signal import find_peaks
import shap
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader, TensorDataset
import plotly.graph_objs as go
# from plotly.subplots import make_subplots
import duckdb
import warnings
warnings.filterwarnings('ignore', category=FutureWarning)  # Ignore known FutureWarnings (e.g., pandas Int64Index)

# Global DuckDB connection (persists between function calls)
con = duckdb.connect(database=':memory:')
start_date = (datetime.today() - timedelta(days=365*10)).strftime('%Y-%m-%d')
end_date = datetime.today().strftime('%Y-%m-%d')
symbol = 'RYCEY'  # Default ticker for testing

def get_date_range(df, date_col='Date'):
    """Safely get first and last dates from DataFrame"""
    # Get dates from column or index
    dates = df[date_col].values if date_col in df.columns else df.index.values
    
    # Convert to datetime and handle empty cases
    dates = pd.to_datetime(dates)
    if len(dates) == 0:
        return None, None
    
    # Use iloc-style access to avoid index issues
    first_date = dates[0].date()
    last_date = dates[-1].date() if len(dates) > 1 else first_date
    
    return first_date, last_date

def calculate_dominant_periods(df, window_size=60):
    """
    Calculate dominant periods using FFT with bias reduction techniques:
    - Proper windowing (Hamming window)
    - Detrending
    - Advanced peak detection
    - Noise floor consideration
    """
    # Initialize results
    dominant_periods = np.full(len(df), np.nan)
    
    # Create window function
    window = np.hamming(window_size)
    
    # Minimum meaningful period (avoid detecting very high frequencies)
    min_period = 2  # Minimum period of 2 samples
    
    for i in range(window_size, len(df)):
        # Get window data and detrend
        window_data = df['Close'].iloc[i - window_size:i].values
        window_data = window_data - np.mean(window_data)  # Remove DC component
        
        # Apply window function
        windowed_data = window_data * window
        
        # Perform FFT
        fft = np.fft.fft(windowed_data)
        freqs = np.fft.fftfreq(len(fft))
        
        # Only consider positive frequencies
        positive_mask = freqs > 0
        positive_freqs = freqs[positive_mask]
        fft_mag = np.abs(fft[positive_mask])
        
        # Convert to power spectrum (magnitude squared)
        power_spectrum = fft_mag**2
        
        # Find all peaks in the power spectrum
        peaks, properties = find_peaks(power_spectrum, height=0)
        
        if len(peaks) > 0:
            # Calculate noise floor (median of non-peak values)
            noise_floor = np.median(power_spectrum[np.setdiff1d(
                np.arange(len(power_spectrum)), peaks
            )])
            
            # Filter peaks that are significantly above noise floor (3dB threshold)
            significant_peaks = peaks[properties['peak_heights'] > 2*noise_floor]
            
            if len(significant_peaks) > 0:
                # Get top 3 significant peaks by magnitude
                top_peaks = significant_peaks[
                    np.argsort(power_spectrum[significant_peaks])[-3:]
                ]
                
                # Select the peak with highest frequency among top peaks
                # (avoids always selecting the lowest frequency peak)
                selected_peak = top_peaks[
                    np.argmax(positive_freqs[top_peaks])
                ]
                
                dom_freq = positive_freqs[selected_peak]
                dom_period = abs(1/dom_freq)
                
                # Validate period is within reasonable bounds
                if dom_period < min_period or dom_period > window_size:
                    dom_period = np.nan
            else:
                dom_period = np.nan
        else:
            dom_period = np.nan
        
        dominant_periods[i] = dom_period
    
    # Post-processing
    df['Dominant_Period'] = dominant_periods
    
    # Linear interpolation for missing values (better than forward fill)
    df['Dominant_Period'] = df['Dominant_Period'].interpolate(
        method='linear',
        limit_area='inside'  # Only fill between valid values
    )
    
    # Optional: Smooth the final series
    df['Dominant_Period'] = df['`Dominant_Per`iod'].rolling(
        window=3,
        center=True,
        min_periods=1
    ).mean()
    
    return df

def clean_column_names(df, ticker):
    """Flatten multi-index columns and remove ticker from column names"""
    # Convert columns to list if they're in the problematic format
    if any(isinstance(col, tuple) for col in df.columns):
        new_columns = []
        for col in df.columns:
            if isinstance(col, tuple):
                # Keep just the metric name (e.g., 'Close') and drop the ticker
                new_columns.append(col[0])
            else:
                new_columns.append(col)
        df.columns = new_columns
    return df

def get_stock_data(ticker: str, force_download: bool = False) -> duckdb.DuckDBPyRelation:
    """Fetch stock data from cache or Yahoo Finance, storing in DuckDB."""
    Path('data').mkdir(exist_ok=True)
    parquet_path = f'data/{ticker}.parquet'
    
    if not force_download and Path(parquet_path).exists():
        print(f"Loading cached data for {ticker}")
        con.execute(f"CREATE OR REPLACE TABLE {ticker}_data AS SELECT * FROM read_parquet('{parquet_path}')").df()
    else:
        print(f"Downloading fresh data for {ticker}")
        df = yf.download(
            ticker, 
            start=(datetime.today() - timedelta(days=365*10)).strftime('%Y-%m-%d'),
            end=datetime.today().strftime('%Y-%m-%d'),
            progress=False
        )
        
        # Clean the column names
        df = clean_column_names(df, ticker)
        
        # Select our standard columns and reset index
        df = df[['Open', 'High', 'Low', 'Close', 'Volume']].dropna()
        df = df[['Open', 'High', 'Low', 'Close', 'Volume']].reset_index()
        
        # Save to Parquet
        df.to_parquet(parquet_path, index=False)
        
        # Register with DuckDB
        con.execute(f"CREATE OR REPLACE TABLE {ticker}_data AS SELECT * FROM df")
    
    return con.table(f'{ticker}_data').df()

# Usage
# if __name__ == "__main__":
# Get data
df = get_stock_data("RYCEY")

# Show schema to verify clean column names
print("Table Schema:")
print(con.execute("DESCRIBE rycey_data").df())

# Query recent data
recent_data = con.execute("""
    SELECT 
        MIN(Date) as start_date
        ,MAX(Date) as end_date
        ,COUNT(*) as row_count
        ,AVG(Close) as avg_close
    FROM df
""").df()

print("\nRecent Data:")
print(recent_data)
print("\nColumn Names:", recent_data.columns.tolist())

# Technical Indicators & Advanced Metrics

# 2.1 Relative Strength Index (RSI) - 14 day
delta = df['Close'].diff()
gain = delta.clip(lower=0)
loss = -delta.clip(upper=0)
avg_gain = gain.ewm(alpha=1/14, adjust=False).mean()
avg_loss = loss.ewm(alpha=1/14, adjust=False).mean()
rs = avg_gain / avg_loss
df['RSI14'] = 100 - (100 / (1 + rs))

# 2.2 Moving Average Convergence/Divergence (MACD: 12,26 with Signal 9)
ema12 = df['Close'].ewm(span=12, adjust=False).mean()
ema26 = df['Close'].ewm(span=26, adjust=False).mean()
df['MACD_line'] = ema12 - ema26
df['MACD_signal'] = df['MACD_line'].ewm(span=9, adjust=False).mean()
df['MACD_hist'] = df['MACD_line'] - df['MACD_signal']

# 2.3 Average True Range (ATR) - 14 day
high_low = df['High'] - df['Low']
high_prev_close = (df['High'] - df['Close'].shift(1)).abs()
low_prev_close  = (df['Low'] - df['Close'].shift(1)).abs()
true_range = pd.DataFrame({'hl': high_low, 'hc': high_prev_close, 'lc': low_prev_close}).max(axis=1)
df['ATR14'] = true_range.rolling(window=14).mean()

# 2.4 Bollinger Bands (20-day SMA Â± 2 STD)
rolling_mean20 = df['Close'].rolling(window=20).mean()
rolling_std20  = df['Close'].rolling(window=20).std()
df['BB_mid']   = rolling_mean20
df['BB_upper'] = rolling_mean20 + 2 * rolling_std20
df['BB_lower'] = rolling_mean20 - 2 * rolling_std20

# 2.5 On-Balance Volume (OBV)
direction = np.sign(df['Close'].diff().fillna(0))
df['OBV'] = (direction * df['Volume']).cumsum()

# 2.6 Stochastic Oscillator %K and %D (14-day)
window = 14
lowest_low  = df['Low'].rolling(window).min()
highest_high = df['High'].rolling(window).max()
df['%K'] = (df['Close'] - lowest_low) / (highest_high - lowest_low + 1e-9) * 100  # +1e-9 to avoid zero division
df['%D'] = df['%K'].rolling(3).mean()

# 2.7 Money Flow Index (MFI) - 14 day
typical_price = (df['High'] + df['Low'] + df['Close']) / 3.0
mf = typical_price * df['Volume']
tp_diff = typical_price.diff()
pos_mf = mf.where(tp_diff > 0, 0.0).rolling(window).sum()
neg_mf = mf.where(tp_diff < 0, 0.0).rolling(window).sum()
df['MFI14'] = 100 - 100 / (1 + pos_mf / (neg_mf + 1e-9))

# 2.8 Commodity Channel Index (CCI) - 20 day
TP20 = typical_price.rolling(20).mean()
MD20 = (typical_price - TP20).abs().rolling(20).mean()  # Mean deviation
df['CCI20'] = (typical_price - TP20) / (0.015 * MD20)

# 2.9 Williams %R (14-day)
df['Williams_%R'] = (highest_high - df['Close']) / (highest_high - lowest_low + 1e-9) * -100

# 2.10 Rate of Change (ROC) - 10 day
df['ROC10'] = df['Close'].pct_change(periods=10) * 100

# 2.11 GARCH(1,1) Volatility Estimate (daily)
returns = df['Close'].pct_change().fillna(0)
# Initialize GARCH parameters (omega, alpha, beta) and variance
var0 = returns.var()
alpha, beta = 0.1, 0.85
omega = var0 * max(0, (1 - alpha - beta))
garch_vars = [var0]
for r in returns.iloc[1:]:
    new_var = omega + alpha * (r**2) + beta * garch_vars[-1]
    garch_vars.append(new_var)
df['GARCH_vol'] = np.sqrt(garch_vars)

# 2.12 Fourier Transform Dominant Period
# Compute the Fourier Transform dominant period for each row using a rolling window
df = calculate_dominant_periods(df, window_size=60)

# 2.13 One-day Return (%) as feature
df['Return1'] = returns * 100

# Drop initial rows with NaN values from rolling calculations
df.dropna(inplace=True)
print(f"After feature engineering: {len(df)} data points, {df.shape[1]} columns (incl. features).")
print(f"{df.Dominant_Period.describe()}\n{df.Close.describe()}")
df.sample(10)  # display last 5 rows of the last 10 feature columns

# Define target: 1 if next day's Close is higher than today's, else 0
df['UpNext'] = (df['Close'].shift(-1) > df['Close']).astype(int)
df.dropna(inplace=True)  # drop last row (no target for it)
print("Target 'UpNext':", df['UpNext'].value_counts().to_dict())  # distribution of up/down

# Train-test split (80% train, 20% test by date order)
train_size = int(0.8 * len(df))
train_df = df.iloc[:train_size].copy()
test_df  = df.iloc[train_size:].copy()
train_start, train_end = get_date_range(train_df, 'Date')
test_start, test_end = get_date_range(test_df, 'Date')

print(f"Training: {train_start} to {train_end} ({len(train_df)} samples)")
print(f"Testing: {test_start} to {test_end} ({len(test_df)} samples)")

# Features for modeling (20+ features we engineered)
feature_cols = [
    'RSI14','MACD_line','MACD_signal','MACD_hist','ATR14',
    'BB_mid','BB_upper','BB_lower','OBV','%K','%D',
    'MFI14','CCI20','Williams_%R','ROC10','GARCH_vol',
    'Dominant_Period','Return1','Close','Volume'
]
X_train = train_df[feature_cols]
y_train = train_df['UpNext'].astype(int)
X_test  = test_df[feature_cols]
y_test  = test_df['UpNext'].astype(int)

# 4.1 Train XGBoost model
xgb_model = XGBClassifier(n_estimators=100, random_state=42, eval_metric='logloss')
xgb_model.fit(X_train, y_train)
y_pred = xgb_model.predict(X_test)
accuracy = (y_pred == y_test).mean()
print(f"XGBoost Test Accuracy: {accuracy:.2%}")

# 4.2 SHAP feature importance for XGBoost
explainer = shap.TreeExplainer(xgb_model)
shap_values = explainer.shap_values(X_test)
# If XGBoost classifier returns a list (one per class), take the second element (positive class)
if isinstance(shap_values, list):
    shap_values = shap_values[1]  # shap values for class "1"
# Calculate mean absolute SHAP value for each feature
importance = np.mean(np.abs(shap_values), axis=0)
feature_importance = pd.Series(importance, index=X_test.columns).sort_values(ascending=False)
print("Top 5 features by SHAP importance:")
print(feature_importance.head(5))

# 5.1 Prepare sequence data for Transformer
window_size = 60  # sequence length (days)
X_values = df[feature_cols].values
y_values = df['UpNext'].values.astype(int)

X_seq, y_seq = [], []
for i in range(window_size, len(X_values)):
    # Sequence of features for days [i-window_size ... i-1]
    X_seq.append(X_values[i-window_size:i])
    # Label for sequence = UpNext of day i-1 (predicting day i relative to i-1)
    y_seq.append(y_values[i-1])
X_seq = np.array(X_seq, dtype=np.float32)
y_seq = np.array(y_seq, dtype=np.int64)

# Split sequence data into train and test sets corresponding to original split
# A sequence ending at index j (label index j) belongs to train if j < train_size, else test
train_seq_idx = np.where((np.arange(window_size, len(X_values)) - 1) < train_size)[0]
test_seq_idx  = np.where((np.arange(window_size, len(X_values)) - 1) >= train_size)[0]
X_seq_train = X_seq[train_seq_idx]
y_seq_train = y_seq[train_seq_idx]
X_seq_test  = X_seq[test_seq_idx]
y_seq_test  = y_seq[test_seq_idx]
print(f"Sequences: {X_seq_train.shape[0]} train sequences, {X_seq_test.shape[0]} test sequences.")

# Convert to torch tensors
X_seq_train_t = torch.tensor(X_seq_train)
y_seq_train_t = torch.tensor(y_seq_train)
X_seq_test_t  = torch.tensor(X_seq_test)
y_seq_test_t  = torch.tensor(y_seq_test)

# 5.2 Define Transformer model (encoder) for binary classification
class StockTransformer(nn.Module):
    def __init__(self, input_features, d_model=64, nhead=4, num_layers=2, num_classes=2):
        super(StockTransformer, self).__init__()
        self.input_features = input_features
        self.d_model = d_model
        # Feature embedding layer: project input features to d_model dimensions
        self.feature_embed = nn.Linear(input_features, d_model)
        # Transformer Encoder layers
        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, batch_first=True)
        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)
        # Final output layer
        self.fc_out = nn.Linear(d_model, num_classes)
    def forward(self, x):
        # x shape: (batch, seq_len, input_features)
        x = x.permute(1, 0, 2)               # -> (seq_len, batch, input_features)
        x = self.feature_embed(x)            # -> (seq_len, batch, d_model)
        x = self.transformer_encoder(x)      # -> (seq_len, batch, d_model)
        out = x[-1, :, :]                    # take the last time step's output: (batch, d_model)
        out = self.fc_out(out)               # -> (batch, num_classes)
        return out

# Initialize model, loss, optimizer
model = StockTransformer(input_features=X_seq_train.shape[2], d_model=64, nhead=4, num_layers=2, num_classes=2)
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

# 5.3 Train the Transformer model
epochs = 5
batch_size = 32
train_dataset = TensorDataset(X_seq_train_t, y_seq_train_t)
train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)

for epoch in range(1, epochs+1):
    model.train()
    total_loss = 0.0
    for X_batch, y_batch in train_loader:
        optimizer.zero_grad()
        outputs = model(X_batch)
        loss = criterion(outputs, y_batch)
        loss.backward()
        optimizer.step()
        total_loss += loss.item() * X_batch.size(0)
    avg_loss = total_loss / len(train_dataset)
    print(f"Epoch {epoch}/{epochs} - Training Loss: {avg_loss:.4f}")

# 5.4 Evaluate on test sequences
model.eval()
with torch.no_grad():
    test_outputs = model(X_seq_test_t)
    test_preds = test_outputs.argmax(dim=1).numpy()
test_accuracy = (test_preds == y_seq_test).mean()
print(f"Transformer Test Accuracy: {test_accuracy:.2%}")

# 6.1 Ensemble probabilities (average of XGBoost and Transformer)
xgb_proba = xgb_model.predict_proba(X_test)[:, 1]            # probability of class 1 from XGBoost
trans_proba = F.softmax(test_outputs, dim=1).numpy()[:, 1]   # probability of class 1 from Transformer
# Ensure we align lengths (Transformer test may have one fewer if sequence window covers until second-last day)
min_len = min(len(xgb_proba), len(trans_proba))
ensemble_proba = (xgb_proba[:min_len] + trans_proba[:min_len]) / 2

# 6.2 Compute Kelly fraction for the latest day in test
latest_p = ensemble_proba[-1]   # ensemble probability of up for the most recent day in test set
kelly_fraction = 2 * latest_p - 1
print(f"Latest ensemble 'Up' probability: {latest_p:.2%}")
print(f"Kelly fraction: {kelly_fraction:.2f}")

# 6.3 Fuzzy logic verdict based on Kelly fraction
if kelly_fraction > 0.5:
    verdict = "Strong Buy"
elif kelly_fraction > 0.1:
    verdict = "Buy"
elif kelly_fraction < -0.5:
    verdict = "Strong Sell"
elif kelly_fraction < -0.1:
    verdict = "Sell"
else:
    verdict = "Hold/Neutral"

print("Fuzzy Verdict for the latest day:", verdict)

df.index = pd.to_datetime(df['Date'])

# fig = make_subplots(rows=1, cols=2)
# 7.1 Price chart with predicted buy/sell signals
fig1 = go.Figure()
# fig1.add_trace(go.Scatter(x=df.index, y=df['Close'], mode='lines', name='Close Price'))
fig1.add_trace(go.Scatter(x=df.index, y=df['Close'], name="Signal", mode="lines"))
# Mark signals in the test period
test_dates = test_df.index  # dates corresponding to X_test
# Align Transformer's test predictions to actual dates (account for window offset)
signal_start_idx = train_df.index[-1]  # last train date
signal_dates = df.index[train_size:]   # dates from start of test
signals = pd.Series(test_preds, index=signal_dates[:len(test_preds)])
# Plot buy signals (predicted up) and sell signals (predicted down)
buy_signals = signals[signals == 1].index
sell_signals = signals[signals == 0].index
fig1.add_trace(go.Scatter(x=buy_signals, y=df.loc[buy_signals, 'Close'], 
                          mode='markers', marker_symbol='triangle-up', 
                          marker_color='green', marker_size=10, name='Predicted Buy'))
fig1.add_trace(go.Scatter(x=sell_signals, y=df.loc[sell_signals, 'Close'], 
                          mode='markers', marker_symbol='triangle-down', 
                          marker_color='red', marker_size=10, name='Predicted Sell'))
fig1.update_layout(title=f"{symbol} Price with Model Signals", 
                   yaxis_title="Price", xaxis_title="Date")

# 7.2 Feature importance bar chart (top 10 features from SHAP)
top_features = feature_importance.head(10)[::-1]  # reverse for plotting (smallest to largest)
fig2 = go.Figure(go.Bar(x=top_features.values, y=top_features.index, orientation='h', marker_color='blue'))
fig2.update_layout(title="Top 10 Feature Importances (SHAP)", xaxis_title="Mean |SHAP|", yaxis_title="Feature")

# 7.3 Display the interactive charts
fig1.show()
fig2.show()
